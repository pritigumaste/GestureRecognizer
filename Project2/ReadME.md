--------------------- Project 2: ------------------------

How to run:

    Run "Project1Part4.py" to open up a drawable canvas. 


-- Application usage and Implementation details:

	a). We have used Visual Studio as our IDE as we are coding in Python.

	b). No code changes have been made in the Project 2. Our main focus was on the analysis of the data collected from 6 users using different input modalities - mouse, touchpad, touchscreen, stylus - each user should draw gestures using all 4 input modalities one after another and per user and per input modality, they will be prompted to add 10 samples for each of the 16 gestures. 

	c). We have performed GHoST and GREAT analysis. There are 4 heatmaps for 4 different input modalities data. For the GREAT analysis we have csv files for all the input modalities. 

	e). We have collected samples from 6 users (1-6) folders which is present in the 'xml_logs_dataset_[Input modality]' sub folders, each represent different inputs data given by all six users. So total 4 sub folders in 'XML_Dataset' main folder. Each sub-folder (for different users) will have 160 XML files (16 gestures x 10 samples) of sample gesture data. So total 960 XML files (160 XML files x 6 user folders) sample gesture dataset for one input modality. So total 3840 XML files in the zip folder. We also added the user consent forms in the 'XML_DataSet' main folder. 


-- We have added a demo video (video_demo.mkv) by showing recognition for a gesture, and presentation involving explanation of the analysis performed.

Team Members:
Sai Mohan Sujay Kanchumarthi - Skanchumarthi@ufl.edu
Priti Gumaste - pgumaste@ufl.edu
